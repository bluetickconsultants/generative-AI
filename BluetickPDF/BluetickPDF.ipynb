{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c59491-e678-45a7-bf0f-e7051019625e",
   "metadata": {},
   "source": [
    "## BluetickPDF - PDF Analyzer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125a4d2-7233-4492-81b5-0e25cf0c1260",
   "metadata": {},
   "source": [
    "<img src=\"bluetick-logo.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "766e3295-b31e-4573-b1ec-20ebcac89e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"add you openai api key\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"add your pinecone api key\"\n",
    "os.environ[\"PINECONE_API_ENV\"] = \"add your pinecone api env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ead32-cb16-4819-8c02-11b5f4d23a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai langchain tiktoken pinecone-client pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678d7d8-5892-41b5-8994-6d741b17bc56",
   "metadata": {},
   "source": [
    "## Loading the PDF: \n",
    "To begin, we need to load the PDF into our system. We can use the PyPDFLoader module from the LangChain library to accomplish this. Here's an example of how to load the PDF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1754884-39ce-4a0e-9f31-2495f1cb2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"The-Book-Thief.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ed45c-498b-4851-b005-d7f9b709ed19",
   "metadata": {},
   "source": [
    "## Splitting the Text: \n",
    "Since the entire book is now loaded as a single document, we need to split it into smaller chunks for processing and querying. The LangChain library provides a RecursiveCharacterTextSplitter that can handle this task. We can define the chunk size and overlap based on our requirements. Here's an example of splitting the book into smaller texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21f63df3-ba40-4be7-9437-da5761d56f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now our book is split up into 353 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=5000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "num_documents = len(texts)\n",
    "print(f\"Now our book is split up into {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819cccf4-73ef-49b3-b450-c2994d2a726a",
   "metadata": {},
   "source": [
    "## Generating Embeddings:\n",
    "To perform efficient queries on the text, we need to generate embeddings for each chunk. Embeddings capture the semantic meaning of the text, enabling us to find similar or relevant chunks efficiently. We can use the OpenAIEmbeddings module from LangChain to generate embeddings. Here's an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1381e4a8-7c12-4abc-a69d-8ce8268aad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87468d8-40ba-4841-9259-d6a5287aa9cf",
   "metadata": {},
   "source": [
    "## Creating a Knowledge Base Index: \n",
    "To enable fast and accurate querying, we will create an index of our knowledge base using Pinecone, a vector search engine. Pinecone allows us to store and retrieve embeddings efficiently. Here's an example of how to create the index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20cb7f0e-314b-43b6-986d-c3843bd73553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=os.environ.get(\"PINECONE_API_KEY\"), environment=os.environ.get(\"PINECONE_API_ENV\"))\n",
    "index_name = \"the-book-thief\"\n",
    "\n",
    "# Create the index\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e90ee-2c6d-48dd-a3d3-45a14ebb9003",
   "metadata": {},
   "source": [
    "## Querying the Knowledge Base:\n",
    " Now that our knowledge base is ready, we can query it using the Generative AI model. We can utilize the ChatOpenAI model from LangChain, which is powered by OpenAI's GPT-3.5 Turbo. Here's an example of how to query the knowledge base:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67a771b7-a7e0-4fb5-9023-21100da5b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, max_tokens=1000, model_name='gpt-3.5-turbo', openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "132dc613-00bc-49ab-b3cd-6f6c178a048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death met the book thief, Liesel Meminger, on two occasions. The first time was when Liesel's younger brother died on a train journey to Munich, and Death came to collect his soul. The second time was when Death took Liesel away from Sydney after she had grown old and died. Death gave Liesel a dusty black book, which turned out to be the book she had written, The Book Thief. Death and Liesel sat down on the curb, and Liesel read her words. Death wanted to tell Liesel many things about beauty and brutality, but all he could do was turn to her and tell her the only truth he truly knows: \"I am haunted by humans.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "index_name = \"the-book-thief\"\n",
    "text_field = \"text\"\n",
    "index = pinecone.Index(index_name)\n",
    "vectorstore = Pinecone(\n",
    "    index, embeddings.embed_query, text_field\n",
    ")\n",
    "\n",
    "query = \"Describe the scenarios when the Death met the book thief\"\n",
    "\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "output = qa.run(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f375d7a-f6ff-4663-a780-5b0235efa750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
